In the [first article](https://www.linkedin.com/pulse/comprehensive-guide-data-engineering-part-six-storage-kenneth-imade-pte0e) in this series on data ingestion we looked at the engineering factors to be considered when designing your data ingestion architecture and also the different patterns used in ingesting data. In this concluding article, we take a look at the technologies which are used in data ingestion and the influence of the undercurrents of the data engineering lifecycle on the data ingestion phase.

### Technologies Used in Data Ingestion

### Direct Database Connection

Data ingestion can be carried out through a connection to a database. This connection is done usually by ODBC or JDBC. ODBC (Open Database Connectivity) and JDBC (Java Database Connectivity) are technologies used to connect to and retrieve data from databases. ODBC employs a driver on the client side to translate application commands into database queries, while JDBC, designed specifically for Java applications, uses a Java driver for a similar purpose, benefiting from Java's cross-platform compatibility. Both are commonly used in data ingestion, where they can execute either many small queries or fewer large ones. However, as they show limitations with complex data types and large-scale operations, modern alternatives like direct file exports in formats like Parquet or ORC, and cloud data warehouse APIs, are increasingly preferred. In practice, JDBC often works in tandem with other technologies for efficient data processing and transfer.

### Change Data Capture

Change Data Capture (CDC) is a technique used for capturing changes in a database, vital for analytics and data replication. There are two main types: Batch-oriented CDC, which periodically checks for updated rows based on a timestamp, and Continuous CDC, which captures changes in real-time, often using a database's log files or managed cloud services. While Batch-oriented CDC is simpler but less comprehensive in capturing all changes, Continuous CDC offers near real-time data updates, ideal for streaming analytics and real-time replication. CDC is also used in database replication, both in synchronous forms (where replicas stay fully in sync with the primary database) and asynchronous forms (which allow for slight delays but offer more flexibility). Implementing CDC requires careful resource management and planning to avoid overloading production systems, making it essential to coordinate with production teams and test thoroughly before deployment.

### APIs

APIs (Application Programming Interfaces) have become a crucial data source for many organizations, but they present challenges due to the lack of a universal standard for data exchange. Data engineers often spend substantial time understanding API documentation, liaising with data owners, and maintaining API connection code. However, three emerging trends are simplifying API integration:

1. **API Client Libraries**: Provided by vendors, these libraries simplify API access by handling much of the underlying complexity.
2. **Data Connector Platforms**: Available as SaaS, open source, or managed solutions, these platforms offer ready-made connectivity to various data sources and allow for custom connector development for unsupported APIs.
3. **Data Sharing Platforms**: Platforms like BigQuery, Snowflake, Redshift, and S3 facilitate straightforward data storage, processing, and transfer, significantly impacting data engineering.

### Message Queues and Event-Streaming Platforms

Message queues and event-streaming platforms are key methods for handling real-time data from sources like web apps, IoT sensors, and smart devices. Message queues process individual data events, removing them once consumed, akin to ticking tasks off a list. Event-streaming platforms, on the other hand, maintain a log of events that can be analyzed and manipulated over time, like a replayable event record. These methods differ from batch processing by being more flexible and dynamic, allowing data to be continuously published, consumed, and republished in various forms. Setting up these real-time data workflows requires ensuring they can handle large volumes of data quickly and efficiently, often necessitating sufficient computing resources and possibly autoscaling. Due to their complexity and resource demands, many opt for managed services to streamline these real-time data ingestion processes.

### Managed Data Connectors

Managed data connector platforms and frameworks, growing in popularity, offer standardized connectors to various data sources. These tools alleviate the need for data engineers to build and maintain complex connections themselves. Using these services, you can simply specify the data source and target, choose the ingestion method (like Change Data Capture, replication, or truncate and reload), set access credentials, configure how often the data should be updated, and start syncing. The management and monitoring of these data syncs are handled by the vendor or cloud service provider. If an issue arises during synchronization, you'll be alerted and provided with details about the error.

### Moving Data with Object Storage

Object storage in public clouds is designed to handle vast amounts of data, making it an excellent choice for various data management tasks. It's particularly useful for transferring data in and out of data lakes, sharing data between different teams or organizations, and managing file exchanges. A key feature of object storage is its ability to provide temporary access to data through signed URLs, which grant short-term permission to users.

The advantages of using object storage for file exchange are numerous. Firstly, it adheres to the latest security standards, ensuring data is stored and transferred securely. Secondly, it has proven to be highly scalable and reliable, capable of handling increasing amounts of data without performance degradation. Additionally, it accepts files of various types and sizes and supports high-performance data movement. These attributes make object storage in public clouds an optimal and secure solution for managing and exchanging large-scale data files.

### EDI

Electronic Data Interchange (EDI) is a concept that data engineers often encounter, referring broadly to various methods of data exchange. Traditionally, EDI might involve somewhat outdated methods like transferring files via email or physical media like flash drives. This is often due to limitations in more modern data transport systems or established human processes within some organizations.

However, data engineers can modernize and streamline EDI processes through automation. For instance, they could set up a cloud-based email server programmed to automatically save received files into the companyâ€™s object storage. This action could then trigger automated workflows to further ingest and process the data. Such automation is significantly more efficient and reliable than the manual process of downloading files from emails and uploading them to internal systems, which is still surprisingly common in many workplaces. Automating EDI in this way helps bridge the gap between older data exchange methods and modern data management practices.

### Databases and File Export

Data engineers need to consider how file exports impact source database systems, especially transactional ones where large data scans can heavily load the database. To minimize performance disruptions, engineers can schedule exports during low-activity periods or break down large exports into smaller, manageable chunks. Another effective strategy is to use read replicas for exports, reducing the load on the primary database. This approach is particularly beneficial for frequent exports during peak times. Additionally, major cloud data warehouses like Snowflake, BigQuery, and Redshift are optimized for direct file exports to object storage, offering efficient and streamlined export processes in various formats.

### Practical Issues with Common File Formats

Data engineers must carefully choose file formats for data export, balancing ubiquity and complexity. CSV, a common format, is prone to errors due to its use of commas as default delimiters and lacks uniformity in encoding schema and nested structures, requiring careful configuration. Advanced formats like Parquet, Avro, Arrow, ORC, or JSON offer better solutions, inherently encoding schema information and efficiently handling complex and nested data. Columnar formats like Parquet, Arrow, and ORC are particularly efficient for columnar databases, optimizing data for query engines and processing. Despite their advantages, these formats are often not natively supported by source systems, leading engineers to start with CSV and implement robust error handling to ensure data quality during ingestion.

### Shell

The shell, a user interface for executing commands, is commonly used for scripting data ingestion workflows. It can automate various tasks, like reading data from a database, converting it to a different format, uploading it to object storage, and triggering ingestion in another database. While effective for smaller data sources, this method has scalability limits for larger datasets. Cloud providers offer command-line interface (CLI) tools that facilitate complex ingestion processes, like using the AWS CLI. However, as ingestion tasks become more complex and service level agreement (SLA) demands increase, moving to a more advanced orchestration system is advisable for better scalability and management.

### SSH

SSH (Secure Shell) is a protocol that supports data ingestion strategies in two main ways:

1. **File Transfer with SCP**: SSH is used with SCP (Secure Copy Protocol) for secure file transfers between local and remote hosts, ensuring safe data movement.
2. **SSH Tunnels for Database Connections**: SSH tunnels provide secure, isolated connections to databases. Instead of exposing databases directly to the internet, engineers use a 'bastion host' â€” a secure, internet-exposed intermediary that only allows connections from specified IP addresses. To access the database, a user first connects to the bastion host via SSH tunnel and then from the host to the database, ensuring secure and controlled database access.

### SFTP and SCP

Familiarity with secure FTP (SFTP) and secure copy (SCP) is essential for data engineers, although these methods are typically managed by IT or security teams. SFTP, despite being somewhat outdated, is still widely used in many business contexts, especially for data exchange with partners who are hesitant to adopt newer standards. It's crucial to conduct thorough security checks when using SFTP to prevent data leaks. SCP, which operates over an SSH connection, can be secure if properly configured. Enhancing SCP security with additional network access controls, part of a 'defence in depth' strategy, is highly recommended. Understanding these protocols is important for data engineers, given their continued prevalence in certain business environments.

### Webhooks

Webhooks, often called reverse APIs, invert the typical dynamic of REST data APIs. Instead of the data consumer making requests to a provider's API, the provider sends data to an API endpoint specified by the consumer. This setup places the responsibility of data ingestion, aggregation, storage, and processing on the data consumer. While webhook-based architectures can be brittle and challenging to maintain, leveraging modern tools can significantly improve robustness and efficiency. For example, in an AWS setup, a combination of Lambda for event reception, Kinesis for message buffering, Flink for real-time analytics, and S3 for storage can create a more effective system. This approach also illustrates the intertwined nature of data ingestion with other aspects of the data engineering lifecycle, like storage and processing.

### Web Interface

Web interfaces for data access, though less efficient, are still commonly encountered by data engineers. In many instances, not all data from a SaaS platform is accessible via automated methods like APIs. Instead, manual processes are required, such as accessing a web interface to generate and download reports. This method has drawbacks, including the possibility of human error or technical failures. Wherever possible, it's advisable to use tools and workflows that support automated data access, reducing reliance on manual, error-prone processes.

### Web Scraping

Web scraping is a common technique in data engineering for extracting data from web pages by parsing HTML elements. However, it's important to consider several factors before engaging in web scraping:

1. **Responsibility and Alternatives**: First, evaluate if the data is accessible through other sources like third-party services. If web scraping is necessary, ensure it's done responsibly to avoid overwhelming the target website or getting blocked, such as by moderating the scraping rate.
2. **Legal Considerations**: Be aware of legal implications. Web scraping can lead to legal issues if it generates a denial-of-service (DoS) attack or violates terms of service.
3. **Maintenance and Use of Data**: Web pages frequently change, which can make maintaining scrapers challenging. Decide whether the effort required to keep scrapers updated is justified. Also, consider how you plan to use the scraped data, as this will influence your data processing architecture.

In short, while web scraping is a useful tool in data engineering, it requires careful ethical, legal, and practical consideration, from the decision to scrape to the maintenance and utilization of the scraped data.

### Transfer Appliances for Data Migration

For transferring massive data amounts (100 TB or more), traditional internet transfer can be slow and expensive. In such cases, the most efficient method is surprisingly old-fashioned: physically shipping the data. Cloud vendors offer services where you can order a physical storage device (a transfer appliance), load it with your data, and then ship it back to the vendor for uploading to their cloud.

### Data Sharing

Data sharing is an emerging method for accessing data, where providers offer datasets to third-party subscribers, either freely or for a fee. These datasets are usually shared in a read-only format, allowing subscribers to integrate them with their own and other third-party datasets. However, this doesn't involve physical possession of the data; access can be revoked by the provider at any time. Many cloud platforms are now supporting this trend, not only allowing for data sharing but also creating data marketplaces where data can be bought and sold. This approach offers a convenient way to access diverse datasets without the complexities of traditional data ingestion but with reliance on the data provider's continued permission.

## Undercurrents of the Data Engineering Lifecycle and Data Ingestion

### Security

When moving data, security vulnerabilities are a key concern, as data is at risk of being intercepted or compromised during transfer. To mitigate these risks:

1. **Within a VPC**: Keep data within the confines of your Virtual Private Cloud (VPC) using secure endpoints to ensure it's not exposed outside this secure environment.
2. **Between Cloud and On-premises**: Use a Virtual Private Network (VPN) or a dedicated private connection for transferring data between cloud and on-premises networks, prioritizing security even if it incurs extra costs.
3. **Over Public Internet**: Ensure data is encrypted when it travels across the public internet, as encryption is crucial for protecting data from unauthorized access during transit.

### Data Management

Data management begins with data ingestion, encompassing considerations like schema changes, ethics, privacy, and compliance:

1. **Schema Changes**: Handling schema changes (like modifying database columns) is a complex issue. Traditional processes are slow, hampering agility, while automatic schema updates in target tables can disrupt downstream systems. A potential solution is to use a Git-like version control approach, maintaining multiple versions of a table to test changes before applying them broadly.
2. **Data Ethics, Privacy, and Compliance**: Essential questions include whether sensitive data is necessary and how to handle it if it is. Avoiding the collection of unneeded sensitive data is key to minimizing risks. When necessary, techniques like tokenization and hashing at ingestion help anonymize data. Working with sensitive data demands high ethical standards and practices that minimize direct handling, like developing and testing with simulated data and implementing touchless production systems.
3. **Handling Sensitive Data**: Simple technological solutions like encryption and tokenization are not always sufficient for complex privacy challenges. The focus should be on controlling access to sensitive data rather than relying solely on encryption. When using tokenization, ensure that it's robust enough to protect individual privacy.

### DataOps

Reliable data pipelines are vital in data engineering, as their failure can disrupt all downstream activities, impacting data warehouses, data lakes, and the work of data scientists and analysts. Ensuring these pipelines are robust involves several key practices:

1. **Monitoring**: Proper monitoring is crucial, especially at the ingestion stage. This includes tracking uptime, latency, and data volumes processed. Effective monitoring helps identify and address failures quickly, preventing prolonged issues.
2. **Response Planning**: Having a plan for responding to ingestion job failures is essential. This should be integrated from the beginning of pipeline development, rather than as an afterthought.
3. **Understanding Upstream Systems**: Being aware of the behaviour of upstream systems, including the frequency and size of events they generate, is important for pipeline design.
4. **Handling Third-party Services**: For third-party services, set up alert systems for outages and develop response plans for service disruptions. Consider failover options to other servers or regions.
5. **Data Quality and Testing**: Bad data can be more detrimental than no data. Itâ€™s important to work with software engineers to address data quality issues at the source. Traditional data testing tools focus on binary logic, but the growing field of statistical data testing is becoming increasingly relevant.

### Orchestration

Ingestion, as the first stage in the data engineering lifecycle, initiates a complex sequence of data processing steps where data from multiple sources intermingles. Effective orchestration is crucial in this context to coordinate these steps.

1. **Early-Stage Approaches**: Initially, organizations may use simple scheduled cron jobs for data ingestion. While this can be effective at first, it's a brittle approach that can limit the speed and development of data engineering processes.
2. **Need for Advanced Orchestration**: As data pipeline complexity increases, a more sophisticated orchestration system becomes essential. Such a system schedules and manages complete task graphs, not just individual tasks, ensuring that each step in the data processing sequence is executed in the right order and at the right time.

### Software Engineering

The ingestion stage in the data engineering lifecycle is highly engineering-intensive and complex, often requiring integration with external systems. This complexity is managed through:

1. **Utilizing Open Source and Proprietary Solutions**: Teams commonly use sophisticated open-source frameworks like Kafka or Pulsar, or bespoke solutions developed by large tech companies.
2. **Leveraging Managed Data Connectors**: Tools like Fivetran, Matillion, and Airbyte have simplified the ingestion process by handling many of the complexities, allowing data engineers to focus on other important tasks.
3. **Adhering to Software Development Best Practices**: It's essential to apply strong software development practices, including version control, code reviews, and thorough testing, even for ingestion-related code.
4. **Writing Decoupled Code**: Ensuring that the code for data ingestion is decoupled from source and destination systems is crucial to avoid inflexible, monolithic systems.

## Conclusion

In conclusion, this guide on mastering data ingestion in data engineering provides an in-depth exploration of the various technologies and methodologies used in the field. It addresses key aspects like direct database connections, Change Data Capture (CDC), API integration, and more contemporary methods such as webhooks and object storage. The guide also delves into practical challenges, emphasizing the importance of security, data management, orchestration, and adherence to software engineering best practices. By covering a wide range of topics, from the technical specifics of data ingestion to overarching concerns such as DataOps and ethical data handling, this guide serves as a valuable resource for data engineers at all levels, offering insights into the complexities and nuances of data ingestion in the modern data landscape.