Understanding how data gets transformed from raw form into useful insights is an important aspect of data engineering. This second part of the series on the transformation stage of the data engineering lifecycle focuses on modelling data.

## Data Modelling

Having a well-thought-out data model is part of the first step in building good data systems. The goals and business logic of an organization should be reflected in its data model.

### What is a Data Model?

A data model is essentially a framework for organizing and interpreting the information that a business collects. It's like a map that shows how different pieces of data are connected and what they mean in the context of the business. This map helps make sure that everyone in the company understands the data in the same way.

### The Stages of Data Modelling

There are three stages of building a data model.

1. **Conceptual Model:** This is the initial stage where you outline the broad structure of the data, focusing on the business logic and rules. It describes the system's data at a high level, like what kinds of data will be stored (e.g., customer names, addresses), without getting into technical details. Entity-Relationship (ER) diagrams are often used here to visualize how different pieces of data (entities) are related (like how customer orders are linked to customer IDs).
2. **Logical Model:** This stage adds more detail to the conceptual model. It involves specifying the types of data (like what format customer IDs or names will be in) and mapping out key relationships between data points (like primary and foreign keys, which are like unique identifiers and links between different data tables).
3. **Physical Model:** The final stage is about implementing the logical model in an actual database system. It involves specifying the exact databases, tables, and configurations that will store and manage the data.

### Normalization

Normalization in databases is a method to organize data efficiently. Let's break it down:

1. **What is Normalization?**Think of it like organizing a messy room. You want to make sure everything is in its place and thereâ€™s no unnecessary clutter. In databases, normalization means arranging data in a way that reduces repetition and ensures each piece of data is stored only once.
2. **Why Normalize Data?**It's like not having duplicate items in your room. If you have the same information in multiple places, it can lead to confusion and errors. Normalization helps keep the data clean and consistent. It also makes it easier to change data. If you have the same data in several places and you need to update it, you'd have to change it everywhere. But with normalization, you just update it once.
3. **History and Objectives:**The concept was introduced by Edgar Codd in the 1970s. The main goals are to avoid problems when adding, updating, or deleting data, to reduce the need to reorganize data as new types are added, to make the database easier to understand, and to ensure the database stays efficient even as the data changes over time.
4. **Normal Forms: Stages of Normalization****Denormalized:** No rules, data can be messy and repetitive. **First Normal Form (1NF):** Each piece of data is unique and exists only once. Each table has a unique identifier, known as a primary key. **Second Normal Form (2NF):** Builds on 1NF. It removes partial dependencies, which means data that doesn't fully depend on the primary key is moved to a different table. **Third Normal Form (3NF):** Builds on 2NF. It ensures that each piece of data in a table is directly related to the primary key and not dependent on other fields.

In simple terms, normalization is about making sure data in a database is tidy, organized, and efficient. It prevents data from being duplicated and makes the database easier to maintain and understand.

### Modelling Techniques for Batch Data

When it comes to modelling batch data for data lakes or warehouses, three techniques are used Inmon, Kimball, and Data Vault.

**Inmon**

Bill Inmon, known as the father of the data warehouse, introduced a data modelling approach in 1989 to improve business decision-making. His concept of a data warehouse centres on creating a separate, specialized database that organizes data by specific subjects like sales or marketing, ensuring it's integrated from various sources, remains unchanged once stored, and includes historical records for time-based analysis. This approach emphasizes the importance of integration, where data is standardized and structured (normalized) to minimize duplication and errors, thus maintaining a "single source of truth." The data warehouse, primarily using an Extract, Transform, Load (ETL) process, feeds into more focused data marts for department-specific analysis, ensuring data reliability and efficiency in organizational decision-making.

![](https://media.licdn.com/dms/image/v2/D4E12AQHAy6MzLFiMow/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1699863088063?e=1731542400&v=beta&t=0rQZFX9tGcBqJWHNSgxiqwPAoLwxw0KSAiziKh_riQA)

Basic Bill Inmon data warehousing architecture explained (Source: Stanford University)

**Kimball**

Ralph Kimball's approach to data modelling, established in the early 1990s, offers a practical, bottom-up method contrasting with Bill Inmon's more structured approach. Kimball emphasizes flexibility and user-friendliness, often accepting denormalization for ease of access and understanding. Central to his method are fact and dimension tables organized in a star schema, where a central fact table (quantitative data) is surrounded by dimension tables (contextual data). This setup facilitates faster queries and simpler interpretation for business users. Kimball also introduces the concept of Slowly Changing Dimensions (SCD) to track changes in data over time, with various types (Type 1, 2, and 3) addressing different needs for historical data retention. The Kimball model is particularly suited for batch data processing and is known for its straightforward implementation in business analytics, albeit with a trade-off in potential data redundancy.

[

![](https://media.licdn.com/dms/image/v2/D4E12AQEfB-eD2sTSgw/article-inline_image-shrink_1500_2232/article-inline_image-shrink_1500_2232/0/1699863836975?e=1731542400&v=beta&t=Xs-mr0_E9OQe2ZpeoPhHBnWuWv1zGVXJvckZ0lsNFJg)



](https://www.geeksforgeeks.org/difference-between-kimball-and-inmon/)

Source: GeeksForGeeks

**Data Vault**

The Data Vault methodology, created by Dan Linstedt in the 1990s, presents a unique approach to data modelling that contrasts with traditional methods like those of Kimball and Inmon. Focused on flexibility and scalability, this methodology is particularly suited for modern, fast-moving data environments. It revolves around three primary table types: hubs, links, and satellites. Hubs store unique business keys and are strictly insert-only to ensure data integrity. Links connect these hubs, mapping relationships between different data elements at a very granular level. Satellites provide detailed contextual information about the data in hubs and links. This structure allows for agile adaptation to changing business needs, with business logic applied at the time of data querying rather than during storage. The Data Vault's adaptability extends to its compatibility with other data modelling techniques and its suitability for various data sources, including NoSQL and streaming data, making it a versatile choice in diverse data landscapes.

[

![](https://media.licdn.com/dms/image/v2/D4E12AQG7FffE5LAAdw/article-inline_image-shrink_1000_1488/article-inline_image-shrink_1000_1488/0/1699864679721?e=1731542400&v=beta&t=2tNniYhYeb_irAu2Q7uWfrsxVg8Vf9xXiicERf0wBy8)



](https://www.analytics.today/blog/when-should-i-use-data-vault)

Data Vault Model (Source: Analytics Today)

### Modelling Techniques for Streaming Data

The shift from batch to streaming data and from on-premises to cloud-based systems is fundamentally altering data modelling practices. Traditional batch data models, like the Kimball or Inmon methods, struggle to adapt to the unbounded and continuously updating nature of streaming data. Experts in streaming data recommend a flexible schema approach, where the data model in the analytical database is not rigid but adaptable to frequent changes from the source. This approach suggests a departure from fixed data models, focusing instead on real-time data accuracy and comprehensive analytics that can handle both streaming and historical data. As the field evolves, future data modelling paradigms are expected to blend traditional analytics with real-time data layers, potentially redefining the distinction between source and analytics systems to better suit the continuous and dynamic nature of modern data.

## Conclusion

In this article we took a look at the importance of data modelling, understanding the various stages of data modelling, why normalization of your database is a good practice and the various ways to model both batch and streaming data. In the next article, we will take a closer look at how we can apply transformations to our data turning it into useful insights.