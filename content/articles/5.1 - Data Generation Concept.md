In this article, we will take a closer look at the first stage of the data engineering lifecycle which is generation. To work with data we need to understand how data is generated and how the systems that generate that data work.

## How Is Data Created?

Data is a bunch of raw facts and numbers that don't have much meaning by themselves. Think of it like a pile of puzzle pieces that haven't been put together yet. This data can be made or collected in lots of different ways.

Some data comes from the regular world around us, like what we say out loud, the hand signals we use, or the music we play. This is called "analogue" data. It's often temporary – like when you talk to someone, and then the words just disappear after the chat.

Other times, data is born in the digital world, which means it's made and used by electronic devices and computers. For example, when you talk to your phone and it turns your words into a text message, that's turning analogue (your voice) into digital (the text). Or when you buy something online with a credit card, the details of what you bought and how much you paid are collected as digital data.

In this section, we're going to look at common ways data is made, like when you do stuff on a website or use an app. But actually, data is being created all the time, all around us – from smart devices in homes, when you pay with a credit card, by scientific instruments, and when people buy and sell stocks.

The important thing is to get to know where your data comes from and how it's made. If you're working with data from a specific system, like a database that organizes data into tables and rows, you should read up on it. Find out how it works, including any special behaviours it might have. This knowledge will help you collect data from these systems more effectively.

## The Various Types Of Source Systems

Data can originate from various sources. Some of these sources that you'll encounter whilst working with data are discussed below:

**Files and Unstructured Data**

A file is a bunch of data that's saved on a computer's storage. Programs write different types of data to files, such as settings, logs, or multimedia like images and audio. Files are also commonly used to exchange data between different systems and people. Despite advances in technology, files remain a popular method for sharing data, like getting spreadsheets from a government agency.

Data engineers often deal with several key file types, including:

- **Excel**: Spreadsheet files used for organizing data in rows and columns.
- **CSV**: Simple text files that use commas to separate values.
- **TXT**: Basic text files with unformatted text.
- **JSON**: Files with a structured format, easy to read for both humans and machines.
- **XML**: Structured files with data tagged in a way similar to web pages.

These files can be well-organized, semi-organized, or unorganized. While there are more advanced file formats for large-scale data handling, it's important to understand these common types as a data engineer.

### APIs

Application Programming Interfaces, or APIs, are like a set of rules that allow different computer programs to talk to each other and swap information. They're supposed to make it easier for people who work with data (data engineers) to grab the data they need. But, in reality, these APIs can still be pretty complicated. Some tools and services try to automate this process, but data engineers frequently find themselves putting in a lot of effort to keep these connections running smoothly.

### Application Databases (OLTP Systems)

An application database keeps track of what's happening in an app, like a bank's system tracking account balances. It's built to handle many quick, small changes made by lots of users at once, a setup known as an OLTP (Online Transaction Processing) system. These systems are speedy at updating individual pieces of data.

Databases operate on a principle called [ACID](https://en.wikipedia.org/wiki/ACID)—ensuring transactions are processed reliably. However, some databases may loosen these rules for better performance, leading to eventual consistency where updates are correct but might take some time to show.

OLTP systems aren't ideal for complex data analysis. Small businesses might start by running analytics on their OLTP systems, but as they grow, this becomes inefficient. Data engineers need to connect OLTP systems to analytics tools without impacting the app's performance.

As apps increasingly combine transaction processing with data analytics, data engineers face challenges in creating systems that can do both efficiently, known as data applications.

### Online Analytical Processing Systems

An OLAP (Online Analytical Processing) system is like a big, powerful computer that's really good at answering complex questions about lots of data at once. It's kind of like having a giant spreadsheet that can help a business understand things like which products sold best over the holiday season for the last five years.

These systems aren't great for quick, small tasks like finding just one piece of data—like looking up what you had for lunch last Tuesday on your food tracking app. They work best when they can look at lots of information at once, not just tiny bits.

So, these OLAP systems are often used in situations where businesses need to analyze trends, make forecasts, or find patterns in their data. But sometimes, the information that's been analyzed and organized by these big systems needs to be used in other ways or in other places—like helping to make decisions about what new products to stock, what marketing strategies to use, or even powering up smart features in apps and services that customers use every day.

### Change Data Capture

Change Data Capture (CDC) is like a surveillance system for a database. Every time someone adds, changes, or removes data, CDC notices and records what happened. It's a way to keep an eye on the data in one place and copy those changes to another place almost instantly, or to send a summary of the changes to another system that might need to react to what's happening. For example, if you're running a website and someone updates their address, CDC could help make sure that change is immediately reflected in the shipping system.

Different types of databases handle CDC in their own ways. Think of it like different security camera setups: some might keep the footage stored locally, while others might upload it directly to the cloud. For traditional databases, CDC might involve keeping a detailed log right where the database is stored. For more modern, cloud-based databases, CDC might mean sending updates straight to a cloud service where they can be used for various needs.

### Logs

A log is like a diary for a computer system, where it writes down everything that happens. When you visit a website, for example, the web server makes a note in its log about your visit, what pages you looked at, and when. This is true for pretty much anything digital - from your own computer's system when you turn it on, to the apps you use, and even to smart devices like your home thermostat.

These logs are incredibly useful because they can tell us stories about how these systems are used and how they perform. Different kinds of tech write logs in different ways:

- Some logs are like shorthand notes, using special codes to save space and make them quick to read and write (binary-encoded logs).
- Others are more like lists with clear labels for each item (semistructured logs, often using a format called JSON).
- And then some logs are just like a stream of thoughts without much structure (plain-text or unstructured logs).

Logs also vary in how detailed they are:

- Some are super detailed, noting down everything so you can replay events just as they happened (high resolution).
- Others only jot down the highlights or summaries (lower resolution).

And they also differ in how fast they report what they see:

- Some logs are like a live broadcast, updating you as things happen (real-time).
- Others are more like a nightly news recap, summarizing the day's events after they've happened (batch logs).

All this log information can be analyzed to figure out patterns, fix problems, or make things more efficient, and that's especially important for people working with big data, machine learning, and automation.

### Database Logs

Think of a database log as a black box recorder for a database. Every time something changes in the database, like when new data is added or existing data is updated, the database quickly jots this down in its log before it says "Okay, I've done that." This log is written in a special computer-friendly code and stored in a safe place.

This process is really important because if something goes wrong with the database, like if it crashes or loses power, this log can be used to remember what it was doing and pick up where it left off, ensuring that no data gets lost in the process.

### CRUD

CRUD is a simple acronym that stands for the four basic actions you can do with data: Create, Read, Update, and Delete.

- **Create** is when you make something new, like a new profile on a social media site.
- **Read** is when you look at or get data that’s already been created, like reading your emails.
- **Update** is when you change that data, like editing your profile picture.
- **Delete** is when you remove the data, like deleting a photo from your album.

This pattern is super common when dealing with databases in apps and websites. It's like the fundamental checklist that ensures an app can manage and keep track of the data it needs. So, when you're using an app, and you save a new contact, search for a friend, change your settings, or remove a message, you're using CRUD operations.

In more technical terms, when a software program wants to handle data in a database, it uses CRUD to do all the essential tasks. When you want to keep an ongoing record of every single one of these actions as they happen, you can use special techniques like Change Data Capture (CDC). This means you can see what changes are being made almost immediately, which is handy for keeping your data analysis up-to-date.

### Insert-Only

The "insert-only" pattern is a way of saving data where every time something changes, instead of changing the existing record, you add a new record with the latest details and a timestamp. This means you never really "delete" or "update" data in the traditional sense; you just keep adding new information.

Imagine you're using an online shopping site and you change your delivery address. If the site uses the insert-only pattern, instead of changing your old address, it adds your new one as a fresh entry. If you want to see what your current address is, the site looks for the most recent entry for your account.

This method is like keeping a diary for data - every change gets a new entry, so you can always look back at the history. It's quite handy if, for example, a bank wants to see all the addresses you've ever used.

However, there are downsides. If data changes a lot, you can end up with a huge amount of it because you're always adding and never taking it away. It can get so big that it's hard to manage or slow to use. To prevent this, sometimes old data is cleaned out, like throwing away old diary entries you don't need anymore.

And if you're trying to find the most current piece of information, it can take a lot of effort, like having to flip through pages of a diary to find the latest entry, which can be slow if there are lots of entries to go through.

### Messages and Streams

In an event-driven architecture, you'll come across two concepts: message queues and streaming platforms. Both are about moving information, but they do it differently.

A message queue is like a post office: messages are sent from one place to another, and once the message is delivered, it's gone from the queue. Imagine a smart thermostat system sending a message to turn on the heat; once the heater gets the message and turns on, that message is done and deleted.

A streaming platform, on the other hand, is more like a video recording of all the day's mail deliveries. It keeps a log of all the messages (events), which stay there for a long time. You can go back and see what happened at any time, like rewinding a video. If you have a bunch of temperature readings from our smart thermostat, a streaming platform lets you look at all that data over time to spot trends or analyze past conditions.

Even though they're different, the systems that handle streams can usually deal with individual messages too. Sometimes you collect all those messages into a stream so you can do more complex analysis later on.

### Types of Time

When you're dealing with data that's constantly being created and sent somewhere to be used—like tweets, stock prices, or weather data—you need to keep track of time very carefully. Three kinds of time matter:

1. **Event Time**: This is the actual moment something happens in the world. Like the exact second someone clicks "buy" on a website. That action has a timestamp, which is its event time.
2. **Ingestion Time**: This is when that click event gets picked up by a system that's going to do something with it, like put it into a database or start processing it. There might be a delay from when the click happened to when the system grabs it—like if you mail a letter, there's a gap between when you write it and when the post office processes it.
3. **Processing Time**: After the event is ingested, something has to happen to it—maybe it's part of a report being generated, or it triggers an order being sent out. Processing time is how long it takes to do that work.

For everything to run smoothly and for you to understand what's going on with your data, you want to keep track of all these times. It's like stamping a timestamp every step of the way, from when an event happens, to when it gets picked up, to when it's fully processed. This helps you figure out if there are delays or issues at any point in the journey of your data.

## Conclusion

In conclusion, we have journeyed through the diverse landscape of data generation, exploring the multifaceted ways in which data is birthed and captured across analog and digital realms. From the ubiquitous presence of files and unstructured data to the complexities of APIs, and from the fast-paced world of OLTP systems to the analytical might of OLAP systems, we have touched upon the dynamic nature of data creation and the critical importance of understanding its origins and nuances.

The advent of CDC, the diligence of logs, the simplicity yet foundational significance of CRUD, the historical tracking of the insert-only pattern, and the innovative communication of messages and streams have all been examined as vital components in the data engineering ecosystem. Moreover, the nuanced understanding of event, ingestion, and processing times has been recognized as pivotal for maintaining the integrity and efficiency of data operations.

While this article has provided a broad overview, the intricacies and real-world applications of these concepts beckon for a more granular exploration. In a forthcoming article, we will delve into illustrative examples that breathe life into these technical elements. We will showcase how these data generation mechanisms function in concert within various industry scenarios, offering a practical perspective that will further demystify the complex yet fascinating world of data engineering. Stay tuned for a continuation of our exploration into the pulse of data's genesis and journey.