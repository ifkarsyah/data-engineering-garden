In the previous article on serving data, we looked at the higher-level tasks which are served. In this article, we dive into the various ways which can be used to serve data for these tasks. We will also take a brief look at reverse ETL and then round up with how the undercurrents of the data engineering lifecycle influence the serving stage.

## Ways To Serve Data

### File Exchange

File sharing is one of the easiest and most common methods of serving data. A simple email and the data consumer has received the requested files. However, when factors such as security, access, versioning, and file size come into play file exchange does not prove wise. At this point, data lakes and object storage are better alternatives.

### Databases

Databases like data warehouses and data lakes are pivotal for effectively serving data. They facilitate structured data organization through schemas and provide robust permission controls, enhancing query management for complex, large-scale analytics. Integrating with Business Intelligence (BI) systems, these databases support varied data processing methods, ranging from local data storage and processing to query pushdown techniques, where BI tools like Looker generate and run SQL queries directly in the database. This integration is crucial for optimizing query performance and managing computational workload. Additionally, in modern data ecosystems, platforms like Snowflake and Databricks offer unified environments, enabling data scientists and analysts to collaboratively work without resource conflicts, thereby ensuring efficient and high-throughput data service to stakeholders.

### Streaming Systems

Streaming analytics involves analyzing data in real-time as it is generated, distinct from traditional static data queries. This approach leverages operational analytics databases, which blend the historical data analysis capabilities of OLAP databases with the immediacy of stream-processing systems. Such databases enable queries across extensive data ranges, from historical to up-to-the-second information. This integration is increasingly vital in analytics and machine learning, as it allows for rapid, informed decision-making and insights by processing continuous data streams. As a result, professionals in these fields are adapting to this paradigm to effectively utilize real-time data for comprehensive analysis and model training.

### Query Federation

Query federation is an evolving approach in data management, allowing for the extraction of data from diverse sources like data lakes, RDBMSs, and data warehouses without centralizing it in an OLAP system. Enabled by tools like Trino, Presto, and Starburst, it facilitates flexible, ad-hoc querying across different systems for exploratory analysis, balancing the need for data control and accessibility. While offering the advantage of read-only access to ensure compliance and data integrity, federated queries pose challenges in managing resource consumption across live production systems. This approach is particularly valuable in scenarios where centralized data management is impractical, and access control is paramount, allowing organizations to decide whether to centralize data based on performance and operational needs.

### Data Sharing

Data sharing represents the exchange of data between different entities, be it organizations or internal units. This approach, facilitated by advanced cloud-based multitenant storage systems, shifts the focus towards security and access control, ensuring safe and authorized data handling. Data sharing is now an integral feature of major cloud data platforms like Snowflake, Redshift, and BigQuery, allowing for secure inter-organizational data exchanges. It empowers data consumers, such as analysts and data scientists, to directly handle and query shared data, making it a versatile and secure method for diverse data serving needs across various organizational structures.

### Semantic and Metrics Layers

While there's a natural inclination to focus on data processing and storage technologies, such as Spark or cloud data warehouses, the quality of data and queries is equally crucial for successful business analytics. The effectiveness of powerful query engines heavily depends on the integrity and logic of the data and queries fed into them. Tools like Looker, with its LookML for defining business logic, and Data Build Tool (dbt), which focuses on SQL data flows and metric standardization, are instrumental in ensuring high-quality ETL processes and query construction. These tools, part of a broader metrics layer, are becoming increasingly essential in analytics, as they help validate data accuracy and consistency, addressing the perennial question in analytics of whether the numbers are indeed correct.

### Serving Data In Notebooks

Data scientists extensively utilize notebooks, like Jupyter Notebook, for various tasks including data exploration, feature engineering, and model training. These notebooks, supporting languages such as Python and R, often start on local machines but can face limitations when dealing with large datasets. To address scalability issues, a shift towards cloud-based notebooks and distributed execution systems like Dask, Ray, or Spark becomes essential. This transition enables handling larger datasets by leveraging the cloud infrastructure's scalability and flexibility. Additionally, data and machine learning engineers play a crucial role in facilitating this shift, managing cloud setup, environment, and operational aspects like version control and access management. In some advanced setups, notebooks are integrated into production workflows, offering a balance between rapid deployment and the robustness of a full production process, depending on the project's value and complexity.

## Reverse ETL

Reverse ETL, a concept that has gained traction in data engineering, refers to the process of transferring processed data from an OLAP database back into the source systems, reversing the traditional ETL flow. This approach, which has become a more formalized part of data engineering in recent years, is particularly useful for integrating valuable insights directly into operational systems, like CRMs, enhancing usability for end-users like sales teams. While various implementation options exist, ranging from custom solutions to off-the-shelf products, the rapidly evolving landscape of reverse ETL demands careful selection. Moreover, it's important to be mindful of potential feedback loops that reverse ETL can create, necessitating robust monitoring and guardrails to prevent issues such as escalating costs or data inaccuracies.

## Undercurrents and Their Influence On The Serving Stage

The undercurrents influence the serving stage in the following ways:

1. **Lifecycle and Serving Data**: Data serving is the culmination of the data engineering lifecycle. It often highlights issues missed in earlier stages and requires a continuous lookout for improvements.
2. **Security in Serving Data**: Security is a crucial aspect of serving data. It involves careful control of data access, adhering to the principle of least privilege, and providing access based on role-specific needs. This includes managing read-only and more advanced access for different user groups and systems, ensuring fine-grained access control, especially in multitenant environments.
3. **Data Management and Trust**: Trustworthy data is essential. Ensuring high-quality, reliable data at the serving stage is critical. This involves active feedback loops with users for reporting problems and requesting improvements, and the use of data obfuscation techniques for compliance and privacy concerns.
4. **DataOps and Monitoring**: DataOps involves operationalizing data management, focusing on monitoring data health, latency, quality, security, version control, and achieving Service Level Objectives (SLOs). Utilizing tools for data observability and traditional DevOps monitoring is crucial.
5. **Data Architecture Considerations**: The architecture should facilitate fast and efficient access to data. Encouraging data scientists to move from local machines to cloud environments for development, testing, and production is important.
6. **Orchestration in Data Serving**: Orchestration is key in managing the complex overlaps in data serving. Decisions about centralized vs. decentralized orchestration impact coordination and gatekeeping. The ownership of orchestration can fall to different teams depending on organizational structure.
7. **Software Engineering and Data Serving**: Serving data has become more straightforward with the proliferation of open-source frameworks and code-first approaches. Data engineers should understand serving interfaces, optimize SQL performance, and be involved in setting up and training teams on Data/MLOps infrastructure. For embedded analytics, collaboration with application developers is necessary to ensure efficient and effective data queries.